{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\n",
    "import os\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU not available, using CPU.')\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "\n",
    "   load_dotenv(\"HF.config\")\n",
    "   hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load feature extractor from pre-trained checkpoint\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"spanish\", task=\"transcribe\")\n",
    "og_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"mevitts/whisper-small-es\")\n",
    "\n",
    "og_model = og_model.to(device)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "# Set model configuration\n",
    "og_model.generation_config = GenerationConfig.from_pretrained(\"openai/whisper-small\")\n",
    "og_model.generation_config.language = \"spanish\"\n",
    "og_model.generation_config.task = \"transcribe\"\n",
    "og_model.config.pad_token_id = og_model.config.eos_token_id\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"spanish\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "\n",
    "def record_audio(duration=5, sample_rate=16000):\n",
    "    \"\"\"Record audio for set duration\"\"\"\n",
    "    print(\"Say what you want\")\n",
    "    myrecording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    \n",
    "    #convert to single array, then normalize audio\n",
    "    audio = myrecording.flatten()\n",
    "    \n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # apply filter to reduce noise\n",
    "    nyquist = sample_rate / 2\n",
    "    cutoff = 100 #Hz\n",
    "    b, a = butter(4, cutoff/nyquist, btype='high')\n",
    "    audio = filtfilt(b, a, audio)\n",
    "    \n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio, sample_rate=16000, model=og_model):\n",
    "    \n",
    "    \n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=sample_rate,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_features = inputs.input_features.to(device)\n",
    "    \n",
    "    predicted_ids = model.generate(\n",
    "        input_features,\n",
    "        max_length=448,  # Increased from default\n",
    "        num_beams=5,     # Increased from default\n",
    "        temperature=0.7  # Slightly reduced from default\n",
    "    )\n",
    "    transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = record_audio()\n",
    "msg = transcribe_audio(rec, model=og_model)\n",
    "matt_msg = transcribe_audio(rec, model=model)\n",
    "print(msg)\n",
    "print(matt_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as gem\n",
    "\n",
    "load_dotenv(\"HF.config\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "#genai.configure(api_key=google_api_key)\n",
    "gem.configure(api_key=google_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = gem.GenerativeModel('gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text):\n",
    "    response = gemini.generate_content(\n",
    "            f\"Eres un asistente conversacional en español. Adaptarse al contexto de la conversación. (Por ejemplo: si la persona te diga algo como si fuera tu amigo o un familiar, respondele con ese rol.) Mantenga la duración de tu respuesta corta. Responde de manera natural y conversacional a: {text}\"\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(\"Cual es tu comida favorita?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_pipeline():\n",
    "    print(\"Recording audio...\")\n",
    "    audio = record_audio()\n",
    "    \n",
    "    print(\"\\nTranscribing audio...\")\n",
    "    transcription = transcribe_audio(audio, model=og_model)\n",
    "    print(f\"Transcription: {transcription}\")\n",
    "    \n",
    "    print(\"\\nGenerating response...\")\n",
    "    response = generate_response(transcription)\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    return transcription, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription, response = test_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from TTS.api import TTS\n",
    "\n",
    "def text_to_speech(text, output_path=\"output.wav\"):\n",
    "    # Initialize TTS with Spanish xtts model\n",
    "    tts = TTS(model_name=\"tts_models/es/css10/vits\", progress_bar=False)\n",
    "    \n",
    "    # Generate speech\n",
    "    tts.tts_to_file(text=text, file_path=output_path)\n",
    "    \n",
    "    # Play the audio\n",
    "    sample_rate, audio = wavfile.read(output_path)\n",
    "    sd.play(audio, sample_rate)\n",
    "    sd.wait()\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/es/css10/vits is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > initialization of speaker-embedding layers.\n",
      " > initialization of language-embedding layers.\n",
      " > Text splitted to sentences.\n",
      "['Mirense a estos pinches gringos!']\n",
      " > Processing time: 0.9494185447692871\n",
      " > Real-time factor: 0.31086182751489044\n"
     ]
    }
   ],
   "source": [
    "output_wav = text_to_speech(\"Mirense a estos pinches gringos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Spanish models:\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: tts_models/multilingual/multi-dataset/xtts_v2\n",
      " 2: tts_models/multilingual/multi-dataset/xtts_v1.1\n",
      " 3: tts_models/multilingual/multi-dataset/your_tts\n",
      " 4: tts_models/multilingual/multi-dataset/bark\n",
      " 5: tts_models/bg/cv/vits\n",
      " 6: tts_models/cs/cv/vits\n",
      " 7: tts_models/da/cv/vits\n",
      " 8: tts_models/et/cv/vits\n",
      " 9: tts_models/ga/cv/vits\n",
      " 10: tts_models/en/ek1/tacotron2\n",
      " 11: tts_models/en/ljspeech/tacotron2-DDC\n",
      " 12: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
      " 13: tts_models/en/ljspeech/glow-tts\n",
      " 14: tts_models/en/ljspeech/speedy-speech\n",
      " 15: tts_models/en/ljspeech/tacotron2-DCA\n",
      " 16: tts_models/en/ljspeech/vits\n",
      " 17: tts_models/en/ljspeech/vits--neon\n",
      " 18: tts_models/en/ljspeech/fast_pitch\n",
      " 19: tts_models/en/ljspeech/overflow\n",
      " 20: tts_models/en/ljspeech/neural_hmm\n",
      " 21: tts_models/en/vctk/vits\n",
      " 22: tts_models/en/vctk/fast_pitch\n",
      " 23: tts_models/en/sam/tacotron-DDC\n",
      " 24: tts_models/en/blizzard2013/capacitron-t2-c50\n",
      " 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
      " 26: tts_models/en/multi-dataset/tortoise-v2\n",
      " 27: tts_models/en/jenny/jenny\n",
      " 28: tts_models/es/mai/tacotron2-DDC [already downloaded]\n",
      " 29: tts_models/es/css10/vits [already downloaded]\n",
      " 30: tts_models/fr/mai/tacotron2-DDC\n",
      " 31: tts_models/fr/css10/vits\n",
      " 32: tts_models/uk/mai/glow-tts\n",
      " 33: tts_models/uk/mai/vits\n",
      " 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      " 35: tts_models/nl/mai/tacotron2-DDC\n",
      " 36: tts_models/nl/css10/vits\n",
      " 37: tts_models/de/thorsten/tacotron2-DCA\n",
      " 38: tts_models/de/thorsten/vits\n",
      " 39: tts_models/de/thorsten/tacotron2-DDC\n",
      " 40: tts_models/de/css10/vits-neon\n",
      " 41: tts_models/ja/kokoro/tacotron2-DDC\n",
      " 42: tts_models/tr/common-voice/glow-tts\n",
      " 43: tts_models/it/mai_female/glow-tts\n",
      " 44: tts_models/it/mai_female/vits\n",
      " 45: tts_models/it/mai_male/glow-tts\n",
      " 46: tts_models/it/mai_male/vits\n",
      " 47: tts_models/ewe/openbible/vits\n",
      " 48: tts_models/hau/openbible/vits\n",
      " 49: tts_models/lin/openbible/vits\n",
      " 50: tts_models/tw_akuapem/openbible/vits\n",
      " 51: tts_models/tw_asante/openbible/vits\n",
      " 52: tts_models/yor/openbible/vits\n",
      " 53: tts_models/hu/css10/vits\n",
      " 54: tts_models/el/cv/vits\n",
      " 55: tts_models/fi/css10/vits\n",
      " 56: tts_models/hr/cv/vits\n",
      " 57: tts_models/lt/cv/vits\n",
      " 58: tts_models/lv/cv/vits\n",
      " 59: tts_models/mt/cv/vits\n",
      " 60: tts_models/pl/mai_female/vits\n",
      " 61: tts_models/pt/cv/vits\n",
      " 62: tts_models/ro/cv/vits\n",
      " 63: tts_models/sk/cv/vits\n",
      " 64: tts_models/sl/cv/vits\n",
      " 65: tts_models/sv/cv/vits\n",
      " 66: tts_models/ca/custom/vits\n",
      " 67: tts_models/fa/custom/glow-tts\n",
      " 68: tts_models/bn/custom/vits-male\n",
      " 69: tts_models/bn/custom/vits-female\n",
      " 70: tts_models/be/common-voice/glow-tts\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: vocoder_models/universal/libri-tts/wavegrad\n",
      " 2: vocoder_models/universal/libri-tts/fullband-melgan [already downloaded]\n",
      " 3: vocoder_models/en/ek1/wavegrad\n",
      " 4: vocoder_models/en/ljspeech/multiband-melgan\n",
      " 5: vocoder_models/en/ljspeech/hifigan_v2\n",
      " 6: vocoder_models/en/ljspeech/univnet\n",
      " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
      " 8: vocoder_models/en/vctk/hifigan_v2\n",
      " 9: vocoder_models/en/sam/hifigan_v2\n",
      " 10: vocoder_models/nl/mai/parallel-wavegan\n",
      " 11: vocoder_models/de/thorsten/wavegrad\n",
      " 12: vocoder_models/de/thorsten/fullband-melgan\n",
      " 13: vocoder_models/de/thorsten/hifigan_v1\n",
      " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
      " 15: vocoder_models/uk/mai/multiband-melgan\n",
      " 16: vocoder_models/tr/common-voice/hifigan\n",
      " 17: vocoder_models/be/common-voice/hifigan\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: voice_conversion_models/multilingual/vctk/freevc24\n",
      "tts_models/es/mai/tacotron2-DDC\n",
      "tts_models/es/css10/vits\n"
     ]
    }
   ],
   "source": [
    "from TTS.utils.manage import ModelManager\n",
    "manager = ModelManager()\n",
    "print(\"Available Spanish models:\")\n",
    "for model in manager.list_models():\n",
    "    if \"es\" in model or \"spanish\" in model.lower():\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement time (from versions: none)\n",
      "ERROR: No matching distribution found for time\n"
     ]
    }
   ],
   "source": [
    "!pip install time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_conversation():\n",
    "    print(\"Starting conversation\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            #record user\n",
    "            print(\"Recording now\")\n",
    "            try:\n",
    "                audio = record_audio()\n",
    "            except Exception as e:\n",
    "                print(f\"Error recording audio: {str(e)}\")\n",
    "                print(\"Please try again.\")\n",
    "                continue\n",
    "            \n",
    "            #transcribe audio\n",
    "            try:\n",
    "                transcription = transcribe_audio(audio, model=og_model)\n",
    "                if not transcription.strip():\n",
    "                    print('No speech detected. Please try again')\n",
    "                    continue\n",
    "                print(f'\\nDid you say: {transcription}?')\n",
    "            except Exception as e:\n",
    "                print(f\"Error recording audio: {str(e)}\")\n",
    "                print(\"Please try again.\")\n",
    "                continue\n",
    "            \n",
    "            #if exit conversation\n",
    "            if transcription.lower() in ['adiós', 'adios', 'chao', 'bye', 'goodbye', 'done', 'stop']:\n",
    "                print(\"\\nEnding conversation. Chao!\")\n",
    "                break\n",
    "            \n",
    "            #generate response\n",
    "            try:\n",
    "                response = generate_response(transcription)\n",
    "            except Exception as e:\n",
    "                    print(f\"Error generating response: {str(e)}\")\n",
    "                    print(\"Please try again.\")\n",
    "                    continue\n",
    "                \n",
    "            \n",
    "            #convert response to speech output\n",
    "            try:\n",
    "                text_to_speech(response.text)\n",
    "            except Exception as e:\n",
    "                    print(f\"Error converting response to speech: {str(e)}\")\n",
    "                    print(f\"Continuing without speech output.\\nResponse: {response.text}\")\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nConversation interrupted by user. ¡Hasta luego!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error: {str(e)}\")\n",
    "            print(\"Please try again.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
