{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "# Call the function\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Set memory optimization environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU and set device\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU not available, using CPU.')\n",
    "# Use the 'device' variable when loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "\n",
    "   load_dotenv(\"HF.config\")\n",
    "   hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import  login\n",
    "\n",
    "#authenticating HF login, as CV17 dataset requires it\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latin American Spanish sample:\n",
      "\n",
      "Dataset sizes:\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "'''\n",
    "# top commented out as anything from now commented out was attempted with common voice, but pivoted to smaller dataset\n",
    "#columns_to_remove = ['client_id', 'path', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
    "\n",
    "#streaming so no fill download, spanish full dataset is 48 GB!\n",
    "ds_train = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_6_0\",\n",
    "    \"es\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ").remove_columns(columns_to_remove)\n",
    "\n",
    "ds_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_6_0\",\n",
    "    \"es\",\n",
    "    split=\"test\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ").remove_columns(columns_to_remove)\n",
    "\n",
    "'''\n",
    "ds_fleur = DatasetDict()\n",
    "columns_to_keep = ['audio', 'transcription']  # These are the essential columns for speech recognition\n",
    "\n",
    "# Load Latin American Spanish data. If need European spanish in future, voxpopuli has almost all europ samples\n",
    "ds_fleur[\"train\"] = load_dataset(\n",
    "    \"google/fleurs\",\n",
    "    \"es_419\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ").select_columns(columns_to_keep)\n",
    "\n",
    "ds_fleur[\"test\"] = load_dataset(\n",
    "    \"google/fleurs\",\n",
    "    \"es_419\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True\n",
    ").select_columns(columns_to_keep)\n",
    "\n",
    "print(\"\\nLatin American Spanish sample:\")\n",
    "#print(ds_fleur[\"train\"][0])\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "#print(f\"Latin American Spanish train set: {len(list(ds_fleur[\"train\"]))} samples\")\n",
    "#print(f\"Latin American Spanish test set: {len(list(ds_fleur[\"test\"]))} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "#load feature extractor from pre-trained checkpoint\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "#load tokenizer, has very extensive byte-pair training\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 los murales o garabatos indeseados reciben el nombre de grafiti\n",
      "Decoded w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>los murales o garabatos indeseados reciben el nombre de grafiti<|endoftext|>\n",
      "Decoded w/out special: los murales o garabatos indeseados reciben el nombre de grafiti\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = ds_fleur[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#verifying tokenizer works\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"spanish\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'train/10005668950815513748.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcription': 'los murales o garabatos indeseados reciben el nombre de grafiti'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_fleur[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "#downsample audio to 16kHz to match that of Whisper's sampling rate\n",
    "ds_fleur = ds_fleur.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'train/10005668950815513748.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcription': 'los murales o garabatos indeseados reciben el nombre de grafiti'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_fleur[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, feature_extractor=feature_extractor, tokenizer=tokenizer):   \n",
    "# resample audio to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # compute log-mel input feats from arrau\n",
    "    inputs = feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask[0]\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f511274f955434ba7da84a90501aeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_fleur = ds_fleur.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=ds_fleur.column_names[\"train\"],\n",
    "    num_proc=4,  # Process 16 samples at a time \n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# gen text conditioned on an input(audio data)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"spanish\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass \n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    # called when collator is used to batch samples together\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # Extract audio features from each sample and pad them to same length\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        #uses processor  built earlier that can call extractor and tokenizer\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Extract text labels from each sample and pad them to same length\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding tokens with -100 (which is ignored in loss calculation)\n",
    "        #when labels attention mask does NotEqual (ne) 1, then returns True. When this is true, meaning padding token,\n",
    "        #then it will replace it with a -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100) \n",
    "\n",
    "        # Remove the start token if it was added during tokenization\n",
    "        # (it will be added again during generation)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        #batch with audio feats and labels\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "- replaces -100 with `pad_token_id` in `label_ids`\n",
    "    - then decodes predicted and label ids to string\n",
    "    - Computes WER\n",
    "'''\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = np.array(pred.label_ids)\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    #two different lines because compute returns dict[Unknown, Unknown] or None. So, first need to get wer val to * by 100\n",
    "    #but before run won't let multiply by None, so need to check if None or a real value\n",
    "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": 100 * wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
    "\n",
    "#define arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-es\",  # change to a repo name of your choice\n",
    "    run_name=\"whisper-spanish-training\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=200,\n",
    "    max_steps=2500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\",\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before training\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "'''\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=ds_fleur[\"train\"],\n",
    "    eval_dataset=ds_fleur[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"HF.config\")\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb_nb_name = os.getenv(\"WANDB_NOTEBOOK_NAME\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu] *",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
