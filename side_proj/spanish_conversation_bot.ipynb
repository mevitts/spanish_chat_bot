{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "# Call the function\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Set memory optimization environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU and set device\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU not available, using CPU.')\n",
    "# Use the 'device' variable when loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "\n",
    "   load_dotenv(\"HF.config\")\n",
    "   hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import  login\n",
    "\n",
    "#authenticating HF login, as CV17 dataset requires it\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latin American Spanish sample:\n",
      "{'audio': {'path': 'train/10005668950815513748.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcription': 'los murales o garabatos indeseados reciben el nombre de grafiti'}\n",
      "\n",
      "Dataset sizes:\n",
      "Latin American Spanish train set: 2796 samples\n",
      "Latin American Spanish test set: 908 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "'''\n",
    "# top commented out as anything from now commented out was attempted with common voice, but pivoted to smaller dataset\n",
    "#columns_to_remove = ['client_id', 'path', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant']\n",
    "\n",
    "#streaming so no fill download, spanish full dataset is 48 GB!\n",
    "ds_train = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_6_0\",\n",
    "    \"es\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ").remove_columns(columns_to_remove)\n",
    "\n",
    "ds_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_6_0\",\n",
    "    \"es\",\n",
    "    split=\"test\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ").remove_columns(columns_to_remove)\n",
    "\n",
    "'''\n",
    "ds_fleur = DatasetDict()\n",
    "columns_to_keep = ['audio', 'transcription']  # These are the essential columns for speech recognition\n",
    "\n",
    "# Load Latin American Spanish data. If need European spanish in future, voxpopuli has almost all europ samples\n",
    "ds_fleur[\"train\"] = load_dataset(\n",
    "    \"google/fleurs\",\n",
    "    \"es_419\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ").select_columns(columns_to_keep)\n",
    "\n",
    "ds_fleur[\"test\"] = load_dataset(\n",
    "    \"google/fleurs\",\n",
    "    \"es_419\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=True\n",
    ").select_columns(columns_to_keep)\n",
    "\n",
    "print(\"\\nLatin American Spanish sample:\")\n",
    "print(ds_fleur[\"train\"][0])\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Latin American Spanish train set: {len(list(ds_fleur[\"train\"]))} samples\")\n",
    "print(f\"Latin American Spanish test set: {len(list(ds_fleur[\"test\"]))} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "#load feature extractor from pre-trained checkpoint\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "#load tokenizer, has very extensive byte-pair training\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 los murales o garabatos indeseados reciben el nombre de grafiti\n",
      "Decoded w/ special:    <|startoftranscript|><|es|><|transcribe|><|notimestamps|>los murales o garabatos indeseados reciben el nombre de grafiti<|endoftext|>\n",
      "Decoded w/out special: los murales o garabatos indeseados reciben el nombre de grafiti\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = ds_fleur[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#verifying tokenizer works\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"spanish\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'train/10005668950815513748.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcription': 'los murales o garabatos indeseados reciben el nombre de grafiti'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_fleur[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "#downsample audio to 16kHz to match that of Whisper's sampling rate\n",
    "ds_fleur = ds_fleur.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'train/10005668950815513748.wav', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'transcription': 'los murales o garabatos indeseados reciben el nombre de grafiti'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_fleur[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, feature_extractor=feature_extractor, tokenizer=tokenizer):\n",
    "    # resample audio to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # compute log-mel input feats from arrau\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeff2e212184b57a54e96cb60ba508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d216412e9abc4ea2bd5cb8fc0c41dbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_fleur = ds_fleur.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=ds_fleur.column_names[\"train\"],\n",
    "    num_proc=4,  # Process 16 samples at a time \n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# gen text conditioned on an input(audio data)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"spanish\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass \n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    # called when collator is used to batch samples together\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # Extract audio features from each sample and pad them to same length\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        #uses processor  built earlier that can call extractor and tokenizer\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Extract text labels from each sample and pad them to same length\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding tokens with -100 (which is ignored in loss calculation)\n",
    "        #when labels attention mask does NotEqual (ne) 1, then returns True. When this is true, meaning padding token,\n",
    "        #then it will replace it with a -100\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100) \n",
    "\n",
    "        # Remove the start token if it was added during tokenization\n",
    "        # (it will be added again during generation)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        #batch with audio feats and labels\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- replaces -100 with `pad_token_id` in `label_ids`\n",
    "    - then decodes predicted and label ids to string\n",
    "    - Computes WER\n",
    "'''\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    #two different lines because compute returns dict[Unknown, Unknown] or None. So, first need to get wer val to * by 100\n",
    "    #but before run won't let multiply by None, so need to check if None or a real value\n",
    "    metric_computed = metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = 100 * metric_computed[\"wer\"] if metric_computed is not None else 0\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
    "\n",
    "#define arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-es\",  # change to a repo name of your choice\n",
    "    run_name=\"whisper-spanish-training\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before training\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "'''\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "\n",
    "'''\n",
    "#since streaming, need to create statefulDataLoader, as can't use streamiing dataset with Seq2SeqqTrainer\n",
    "#because trainer needs regular dataset\n",
    "train_dataloader = StatefulDataLoader(\n",
    "    ds_train, \n",
    "    batch_size=training_args.per_device_train_batch_size,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = StatefulDataLoader(\n",
    "    ds_test,\n",
    "    batch_size=training_args.per_device_eval_batch_size,\n",
    "    num_workers=4\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=ds_fleur[\"train\"],\n",
    "    eval_dataset=ds_fleur[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattevitts\u001b[0m (\u001b[33mmattevitts-auburn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\marte\\OneDrive\\Desktop\\Nueva_carpeta\\Python\\side_proj\\wandb\\run-20250527_095736-smahdksa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mattevitts-auburn-university/huggingface/runs/smahdksa' target=\"_blank\">whisper-spanish-training</a></strong> to <a href='https://wandb.ai/mattevitts-auburn-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mattevitts-auburn-university/huggingface' target=\"_blank\">https://wandb.ai/mattevitts-auburn-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mattevitts-auburn-university/huggingface/runs/smahdksa' target=\"_blank\">https://wandb.ai/mattevitts-auburn-university/huggingface/runs/smahdksa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='361' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 361/5000 2:27:10 < 31:41:49, 0.04 it/s, Epoch 2.06/29]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"HF.config\")\n",
    "wandb_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb_nb_name = os.getenv(\"WANDB_NOTEBOOK_NAME\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
